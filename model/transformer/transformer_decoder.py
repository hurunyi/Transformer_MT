import math
from typing import Any, Dict, List, Optional, Tuple

import torch.nn as nn
from model.utils import Dropout, PositionalEmbedding

from model.transformer import transformer_layer
from torch import Tensor
import torch
from torch.nn import functional as F


class TransformerDecoderBase(nn.Module):
	def __init__(
			self,
			dropout,
			dictionary,
			embed_tokens,
			no_encoder_attn=False,
			output_projection=None,
	):
		super().__init__()
		self.dictionary = dictionary
		self._future_mask = torch.empty(0)

		self.dropout_module = Dropout(dropout)
		self.share_input_output_embed = True

		input_embed_dim = embed_tokens.embedding_dim
		embed_dim = 512
		self.embed_dim = embed_dim
		self.output_embed_dim = 512

		self.padding_idx = embed_tokens.padding_idx
		self.max_target_positions = 1024

		self.embed_tokens = embed_tokens

		self.embed_scale = math.sqrt(embed_dim)

		self.project_in_dim = (
			Linear(input_embed_dim, embed_dim, bias=False)
			if embed_dim != input_embed_dim
			else None
		)
		self.embed_positions = (
			PositionalEmbedding(
				self.max_target_positions,
				embed_dim,
				self.padding_idx
			)
		)
		self.layernorm_embedding = None

		self.layers = nn.ModuleList([])
		self.layers.extend(
			[
				self.build_decoder_layer(dropout, no_encoder_attn)
				for _ in range(6)
			]
		)
		self.num_layers = len(self.layers)

		self.layer_norm = None

		self.project_out_dim = (
			Linear(embed_dim, self.output_embed_dim, bias=False)
			if embed_dim != self.output_embed_dim else None
		)

		self.output_projection = output_projection
		if self.output_projection is None:
			self.build_output_projection(dictionary)

	@staticmethod
	def get_normalized_probs(
			net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],
			log_probs: bool,
			sample: Optional[Dict[str, Tensor]] = None,
	):
		"""Get normalized probabilities (or log probs) from a net's output."""
		logits = net_output[0]
		if log_probs:
			return F.log_softmax(logits, dim=-1, dtype=torch.float32)
		else:
			return F.softmax(logits, dim=-1, dtype=torch.float32)

	def build_output_projection(self, dictionary):
		if self.share_input_output_embed:
			self.output_projection = nn.Linear(
				self.embed_tokens.weight.shape[1],
				self.embed_tokens.weight.shape[0],
				bias=False,
			)
			self.output_projection.weight = self.embed_tokens.weight
		else:
			self.output_projection = nn.Linear(
				self.output_embed_dim, len(dictionary), bias=False
			)
			nn.init.normal_(
				self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5
			)

	@staticmethod
	def build_decoder_layer(dropout, no_encoder_attn=False):
		layer = transformer_layer.TransformerDecoderLayerBase(dropout, no_encoder_attn)
		return layer

	def forward(
			self,
			prev_output_tokens,
			encoder_out: Optional[Dict[str, List[Tensor]]] = None,
			incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
			features_only: bool = False,
			full_context_alignment: bool = False
	):
		"""
		Args:
			prev_output_tokens (LongTensor): previous decoder outputs of shape
				`(batch, tgt_len)`, for teacher forcing
			encoder_out (optional): output from the encoder, used for
				encoder-side attention, should be of size T x B x C
			incremental_state (dict): dictionary used for storing state during
				:ref:`Incremental decoding`
			features_only (bool, optional): only return features without
				applying output layer (default: False).
			full_context_alignment (bool, optional): don't apply
				auto-regressive mask to self-attention (default: False).

		Returns:
			tuple:
				- the decoder's output of shape `(batch, tgt_len, vocab)`
				- a dictionary with any model-specific outputs
		"""

		x, extra = self.extract_features(
			prev_output_tokens,
			encoder_out=encoder_out,
			incremental_state=incremental_state,
			full_context_alignment=full_context_alignment
		)

		if not features_only:
			x = self.output_layer(x)
		return x, extra

	def extract_features(
			self,
			prev_output_tokens,
			encoder_out: Optional[Dict[str, List[Tensor]]],
			incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
			full_context_alignment: bool = False
	):
		bs, slen = prev_output_tokens.size()
		alignment_layer = self.num_layers - 1

		enc: Optional[Tensor] = None
		padding_mask: Optional[Tensor] = None
		if encoder_out is not None and len(encoder_out["encoder_out"]) > 0:
			enc = encoder_out["encoder_out"][0]
			assert (enc.size()[1] == bs), f"Expected enc.shape == (t, {bs}, c) got {enc.shape}"
		if encoder_out is not None and len(encoder_out["encoder_padding_mask"]) > 0:
			padding_mask = encoder_out["encoder_padding_mask"][0]

		# embed positions
		positions = None
		if self.embed_positions is not None:
			positions = self.embed_positions(
				prev_output_tokens, incremental_state=incremental_state
			)

		if incremental_state is not None:
			prev_output_tokens = prev_output_tokens[:, -1:]
			if positions is not None:
				positions = positions[:, -1:]

		# embed tokens and positions
		x = self.embed_scale * self.embed_tokens(prev_output_tokens)

		if self.project_in_dim is not None:
			x = self.project_in_dim(x)

		if positions is not None:
			x += positions

		if self.layernorm_embedding is not None:
			x = self.layernorm_embedding(x)

		x = self.dropout_module(x)

		# B x T x C -> T x B x C
		x = x.transpose(0, 1)

		self_attn_padding_mask: Optional[Tensor] = None
		if prev_output_tokens.eq(self.padding_idx).any():
			self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)  # batch x tgt_len

		# decoder layers
		attn: Optional[Tensor] = None
		inner_states: List[Optional[Tensor]] = [x]
		for idx, layer in enumerate(self.layers):
			if incremental_state is None and not full_context_alignment:
				self_attn_mask = self.buffered_future_mask(x)
			else:
				self_attn_mask = None

			x, layer_attn, _ = layer(
				x,
				enc,
				padding_mask,
				incremental_state,
				self_attn_mask=self_attn_mask,
				self_attn_padding_mask=self_attn_padding_mask,
				need_attn=bool((idx == alignment_layer)),
				need_head_weights=bool((idx == alignment_layer)),
			)
			inner_states.append(x)
			if layer_attn is not None and idx == alignment_layer:
				attn = layer_attn.float().to(x)

		if attn is not None:
			# average probabilities over heads
			attn = attn.mean(dim=0)

		if self.layer_norm is not None:
			x = self.layer_norm(x)

		# T x B x C -> B x T x C
		x = x.transpose(0, 1)

		if self.project_out_dim is not None:
			x = self.project_out_dim(x)

		return x, {"attn": [attn], "inner_states": inner_states}

	def output_layer(self, features):
		"""Project features to the vocabulary size."""
		# project back to size of vocabulary
		return self.output_projection(features)

	def set_beam_size(self, beam_size):
		"""Sets the beam size in the decoder and all children."""
		if getattr(self, "_beam_size", -1) != beam_size:
			seen = set()

			def apply_set_beam_size(module):
				if (
						module != self
						and hasattr(module, "set_beam_size")
						and module not in seen
				):
					seen.add(module)
					module.set_beam_size(beam_size)

			self.apply(apply_set_beam_size)
			self._beam_size = beam_size

	def max_positions(self):
		"""Maximum output length supported by the decoder."""
		if self.embed_positions is None:
			return self.max_target_positions
		return min(self.max_target_positions, self.embed_positions.max_positions)

	def buffered_future_mask(self, tensor):
		def fill_with_neg_inf(t):
			"""FP16-compatible function that fills a tensor with -inf."""
			return t.float().fill_(float("-inf")).type_as(t)

		dim = tensor.size(0)
		if (
				self._future_mask.size(0) == 0
				or (not self._future_mask.device == tensor.device)
				or self._future_mask.size(0) < dim
		):
			self._future_mask = torch.triu(
				fill_with_neg_inf(torch.zeros([dim, dim])), 1
			)
		self._future_mask = self._future_mask.to(tensor)
		return self._future_mask[:dim, :dim]

	def reorder_incremental_state_scripting(
			self,
			incremental_state: Dict[str, Dict[str, Optional[Tensor]]],
			new_order: Tensor,
	):
		"""Main entry point for reordering the incremental state.
		Due to limitations in TorchScript, we call this function in
		:class:`fairseq.sequence_generator.SequenceGenerator` instead of
		calling :func:`reorder_incremental_state` directly.
		"""
		for module in self.modules():
			if hasattr(module, "reorder_incremental_state"):
				result = module.reorder_incremental_state(incremental_state, new_order)
				if result is not None:
					incremental_state = result


def Linear(in_features, out_features, bias=True):
	m = nn.Linear(in_features, out_features, bias)
	nn.init.xavier_uniform_(m.weight)
	if bias:
		nn.init.constant_(m.bias, 0.0)
	return m
