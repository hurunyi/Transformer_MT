from torch.nn.modules import Module


def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):
	if target.dim() == lprobs.dim() - 1:
		target = target.unsqueeze(-1)
	nll_loss = -lprobs.gather(dim=-1, index=target)
	smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
	if ignore_index is not None:
		pad_mask = target.eq(ignore_index)
		nll_loss.masked_fill_(pad_mask, 0.0)
		smooth_loss.masked_fill_(pad_mask, 0.0)
	else:
		nll_loss = nll_loss.squeeze(-1)
		smooth_loss = smooth_loss.squeeze(-1)
	if reduce:
		nll_loss = nll_loss.sum()
		smooth_loss = smooth_loss.sum()
	eps_i = epsilon / (lprobs.size(-1) - 1)
	loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
	return loss, nll_loss


class LabelSmoothedCrossEntropyCriterion(Module):
	def __init__(self, label_smoothing):
		super().__init__()
		self.eps = label_smoothing

	def forward(self, model, sample, reduce=True):
		"""Compute the loss for the given sample.

		Returns a tuple with three elements:
		1) the loss
		2) logging outputs to display while training
		"""
		net_output = model(**sample["net_input"])
		loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
		logging_output = {
			"loss": loss.data,
			"nll_loss": nll_loss.data,
			"ntokens": sample["ntokens"],
			"nsentences": sample["target"].size(0),
		}
		return loss, logging_output

	def get_lprobs_and_target(self, model, net_output, sample):
		lprobs = model.get_normalized_probs(net_output, log_probs=True)
		target = model.get_targets(sample, net_output)
		return lprobs.view(-1, lprobs.size(-1)), target.view(-1)

	def compute_loss(self, model, net_output, sample, reduce=True):
		lprobs, target = self.get_lprobs_and_target(model, net_output, sample)
		loss, nll_loss = label_smoothed_nll_loss(
			lprobs,
			target,
			self.eps,
			reduce=reduce,
		)
		return loss, nll_loss
